{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Basics\n",
    "\n",
    "The package `h5wrapper` adds additional functionalities to the core HDF5-interface, which is implemented in the package `h5py` (https://docs.h5py.org/en/stable/). This is done by providing so-calle wrapper classes. They\n",
    " - facilitate and streamline the work with HDF5 files and\n",
    " - integrate meta-conventions and file-layout specifications.\n",
    "\n",
    "Besides high-level methods that enhances the usability, naming `conventions`, the usage of `units` and the defintion of so-called `layouts` are a core feature. They are motivated by the FAIR principles of sustainable data management.\n",
    "\n",
    "This notebook will guide through the high-level methods and the mentioned concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5rdmtoolbox as h5tbx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a HDF file\n",
    "\n",
    "To work with HDF5 files, the \"wrapper\"-class `H5File` is used (equiilant to `h5py.File`).\n",
    "\n",
    "There are several ways to open a file with this package.<br>\n",
    "First thing to note, is that we don't need to specify a filename when calling the the class. Instead a temporary file will be generated. Like this, the default mode is `r+`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5 = h5tbx.H5File()  # note, not passing `mode` while the filename does not exist: 'r+' is used, otherwise defaultis 'r'\n",
    "h5.close()\n",
    "print(h5.hdf_filename.name)  # equal to h5.filename but a pathlib.Path and exists also after file is closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that we have an additional property `hdf_filename`, which in contrast to `filename` works also even if the clss is closed.\n",
    "\n",
    "**A safer way to work with files** is to use python's context manager. This is highly recommended and used throughout the hole documentation and package.\n",
    "\n",
    "Thus, the above cell changes to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File('test.hdf', 'w') as h5:\n",
    "    print(h5.hdf_filename.name)\n",
    "h5.hdf_filename.unlink()  # it's a pathlib.Path object, thus easy to delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default content and content exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever a file is created, some default attributes are written to it. When can check the content by \"dumping\" the content to the screen by callind `dump()`. An interactive html representation of the file content is displayed. At the moment only attributes at the root level exist. They were created when the file was opened in write mode. While `creation_time` is considered as regular \"attribute\" `__h5rdmtoolbox_version__` and `__wrcls__` - attributes starting and ending with `__` - are special attributes reserved/used by the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File() as h5file:  # default mode is 'r+'\n",
    "    h5file.dump()   # call .sdump() outside of notebooks to get a similar but non-interactive representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registration of properties\n",
    "To facilitate and speed up certain tasks and workflows, it might be usefull to have additional properties. Let's assume we want to store a username as an attribute in the HDF5 file. Let's further assume that a user name has a firstname and a surname and both must start with a capitalized letter. To catch possible errerneous input by the user, we would need an additional method, e.g. \"set_username\" or a property \"username\" of `H5File`.<br>**Two** possible ways to achieve this: **inheritance or composition**. The quick and recommended way to add a new class property is to use composition by \"registering\" the property. Like this we don't have to rewrite (inherite at least) the API but only simply write the porperty class.<br>\n",
    "Let's first write a property class \"username\". It needs **three methods**: `get`, `set` and `delete`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@h5tbx.h5wrapper.register_special_property(h5tbx.H5File, overwrite=True)\n",
    "class username:\n",
    "    \n",
    "    def get(self):\n",
    "        _username = self.attrs.get('username', None)\n",
    "        if _username is not None:\n",
    "            return _username\n",
    "        raise AttributeError('No user found')\n",
    "        \n",
    "    def set(self, _username):\n",
    "        \"\"\"Write the user name to HDF attribute of naming convention is matched\"\"\"\n",
    "        _username_split = _username.split(' ')\n",
    "        if not len(_username_split) == 2:\n",
    "            raise ValueError(f'User name must have first name and surname spearated by a space, but got: {_username}')\n",
    "        if _username_split[0][0].islower() or _username_split[0][0].islower():\n",
    "            raise ValueError(f'Names must have capitalized first letters, but got: {_username}')\n",
    "        self.attrs.create('username', _username)\n",
    "        \n",
    "    def delete(self):\n",
    "        _username = self.attrs.get('username', None)\n",
    "        if _username:\n",
    "            print(f\"deleting '{_username}'\")\n",
    "            self.attrs.__delitem__('username')\n",
    "        else:\n",
    "            raise AttributeError('No user to be deleted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words about above lines:\n",
    "- `@h5tbx.h5wrapper.register_special_property(h5tbx.H5File)`: registering the below class (only) to the class `H5File`\n",
    "- don't use existing property names. An error will be raised anyhow. You may however pass `overwrite=True` in the registration method. Be careful though!\n",
    "- provide `set` and optionally also `get` and `delete`. The method `get` makes sense though, while `delete` is not really needed most of the times.\n",
    "\n",
    "Let's check if it worked out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File() as h5file:\n",
    "    try:\n",
    "        h5file.username = 'adam Username'\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "    h5file.username = 'Adam Username'\n",
    "    print('The user name is: ', h5file.attrs['username'])\n",
    "    \n",
    "    del h5file.username\n",
    "    try:\n",
    "        del h5file.username\n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "        \n",
    "    h5file.attrs['username'] = 'lower lower'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# (Naming) Conventions\n",
    "\n",
    "To meet the sustainable (FAIR) principles of data management, the package introduces conventions, that define not only which properties must be attached to a data set but also what naming is allowed. Specifically, the following attributes are obligatory with HDF5 dataset:\n",
    " - `units`\n",
    " - `standard_name`\n",
    " - `long_name` (not needed if `standard_name` exists and vise versa)\n",
    "\n",
    "**units**<br>\n",
    "We expect that each data set written to the HDF5 file has a physical unit or no unit at all. It is registered in the attribute `units`.\n",
    "\n",
    "**standard_name and long_name**<br>\n",
    "For the sake of improved readability and interpretability we suggest to use `long_name` or `standard_name` as additional attributes. While `long_name` is human-readable and interpretable attribute, `standard_name` is intended to be read by a machine (other software). This allows to automate exploration and processing work.\n",
    "\n",
    "The `standard_name` generally should not be chosen freely but must follow a certain convention. Such a naming convention may be defined by a project or a community, e.g. the climate and forecast community [cfconventions.org], from which the concept of standard names is adoped. A convention is described in an **XML** file and associated with the wrapper file `H5File`. Again this is adoped from [cfconventions.org]. The XML file contains the standard name, a description for each one and the respective unit. Thus, if a convention is associated with a wrapper class, the standard name and unit cannot be freely be chsen but is verified.\n",
    "\n",
    "[cfconventions.org]: http://cfconventions.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look into one of the implemented conventions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "h5tbx.conventions.FluidStandardNameTable.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks\n",
    "We can use the standard name table to check some name. We can check for name compliance in a strict or non-strict way. That means, that we check whether the name actally exists in the table or if only the naming conventn in a formal way is approved (e.g. does not start with a letter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    h5tbx.conventions.FluidStandardNameTable.check_name('test', strict=True)\n",
    "except h5tbx.conventions.StandardizedNameError as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    h5tbx.conventions.FluidStandardNameTable.check_name('1234test', strict=False)\n",
    "except h5tbx.conventions.StandardizedNameError as e:\n",
    "    print(e)\n",
    "    \n",
    "print(h5tbx.conventions.FluidStandardNameTable.check_name('test', strict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h5tbx.conventions.FluidStandardNameTable.check_name('x_velocity', strict=False))\n",
    "\n",
    "try:\n",
    "    h5tbx.conventions.FluidStandardNameTable.check_units('test', 'm')\n",
    "except h5tbx.conventions.StandardizedNameError as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    h5tbx.conventions.FluidStandardNameTable.check_units('x_velocity', 'm')\n",
    "except h5tbx.conventions.StandardizedNameError as e:\n",
    "    print(e)\n",
    "    \n",
    "h5tbx.conventions.FluidStandardNameTable.check_units('x_velocity', 'm/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation\n",
    "\n",
    "As motivated, the package enforces us to use certain meta information to meet the FAIR principles. A dataset creation as known from the `h5py` package is therefore not possible, because we have to pass `units` and `standard_name` or `long_name`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File(standard_name_table=None) as h5:\n",
    "    try:\n",
    "        h5.create_dataset('x', shape=(4,))\n",
    "    except h5tbx.conventions.UnitsError as e:\n",
    "        print(e)\n",
    "    h5.create_dataset('x', shape=(4,), units='m', long_name='a coordinate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we only used a long name. What about standard name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File(standard_name_table=None) as h5:\n",
    "    h5.create_dataset('x', shape=(4,), units='m', standard_name='a coordinate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No problem so far because standard names are not regulated yet since we did not specify a `convention` with the `H5File`-object. In fact we even passed `standard_name_table=None`.\n",
    "\n",
    "Let's pass the already implemented fluid convention to the wrapper class (The convention is motivated once again from the cf-conventions). We run through various errors first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File(standard_name_table=h5tbx.conventions.FluidStandardNameTable) as h5:\n",
    "    try:\n",
    "        h5.create_dataset('x', shape=(4,), units='m', standard_name='a coordinate')\n",
    "    except h5tbx.conventions.StandardizedNameError as e:\n",
    "        print(e)\n",
    "    \n",
    "    try:\n",
    "        h5.create_dataset('x', shape=(4,), units='m', standard_name='a_coordinate')\n",
    "    except h5tbx.conventions.StandardizedNameError as e:\n",
    "        print(e)\n",
    "    \n",
    "    try:\n",
    "        h5.create_dataset('x', shape=(4,), units='kg', standard_name='x_coordinate')  # note the wrong units!\n",
    "    except h5tbx.conventions.StandardizedNameError as e:\n",
    "        print(e)\n",
    "        \n",
    "    h5.create_dataset('x', shape=(4,), units='m', standard_name='x_coordinate')  # not finally correct\n",
    "    h5.create_dataset('y', shape=(4,), units='km', standard_name='y_coordinate')  # only base units is checked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced dataset creation\n",
    "\n",
    "There is more to dataset creation. You can:\n",
    "- add attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File() as h5:\n",
    "    h5.create_dataset('ds', shape=(10, ), units='', attrs=dict(long_name='a long name', anothera='another attr'))  # unitless dataset. long_name is passed via parameter attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make and attach scales (Note the output using `dump()`: the scale \"link\" is shown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File() as h5:\n",
    "    h5.create_dataset('x', data=[1,2,3], units='m', standard_name='x_coordinate', make_scale=True)\n",
    "    h5.create_dataset('t', data=[20.1, 18.5, 24.7], units='degC', standard_name='temperature', attach_scale=h5['x'])\n",
    "    h5.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- add `xarry.DataArrays`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "arr =  xr.DataArray(dims=('y', 'x'), data=np.random.rand(3, 2),\n",
    "                                 coords={'y': xr.DataArray(dims='y', data=[1, 2, 3],\n",
    "                                                               attrs={'units': 'm',\n",
    "                                                                      'standard_name': 'y_coordinate'}),\n",
    "                                         'x': xr.DataArray(dims='x',\n",
    "                                                               data=[0, 1],\n",
    "                                                               attrs={'standard_name': 'x_coordinate'})\n",
    "                                        },\n",
    "                                 attrs={'long_name': 'a long name',\n",
    "                                        'units': 'm/s'})\n",
    "\n",
    "with h5tbx.H5File() as h5:\n",
    "    h5.create_dataset('temperature', data=arr)\n",
    "    h5.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset slicing\n",
    "\n",
    "Differently to the `h5py` package, `xarray.DataArray` is returned and not `numpy.ndarray`. Note, how also the attach dimension scales are considered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File() as h5:\n",
    "    dsx = h5.create_dataset('x', data=np.linspace(0, 10, 5), units='mm', long_name='x', make_scale=True)\n",
    "    dsy = h5.create_dataset('y', data=np.linspace(0, 5, 3), units='mm', long_name='y', make_scale=True)\n",
    "    h5.create_dataset('data', data=np.random.random((3, 5)), units='m/s', long_name='velocity', attach_scales=(dsy, dsx))\n",
    "    data_arr = h5['data'][:]\n",
    "data_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group creation\n",
    "Is not much different except that you can pass a `long_name` and additional attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File() as h5:\n",
    "    h5.create_group('mygrp')\n",
    "    h5.create_group('othergrp', long_name='my other group', attrs=dict(one=2, two='a second attr'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta convention: Static layouts:\n",
    "\n",
    "Each HDF wrapper class must fulfill certain requirements, e.g.\n",
    " - to have specific attributes\n",
    " - to have specific attributes with a certain value\n",
    " - to have specific groups\n",
    " - to have specific datasets\n",
    " - to have a specific dataset with a specific shape\n",
    "\n",
    "The basic wrapper-class reflects the \"minimum standard\". It must have certain atributes, such as \"creation_time \" or \"title\". To check which one exactly, let's print the content of the file to the screen using `dump()` (outside of jupyter notebooks print the instance or call `sdump()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File(h5tbx.generate_temporary_filename(suffix='.hdf'), 'w') as h5:\n",
    "    h5.attrs['title'] = 'Tutorial data'\n",
    "    h5.attrs['__version__'] = '0.1.12'\n",
    "    test_filename = h5.filename  # or h5.hdf_filename\n",
    "\n",
    "with h5tbx.H5File(test_filename) as h5:\n",
    "    h5.check(silent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "As the return value of a sliced dataset is a `xarray.DataArray` instead of a `numpy.ndarray` plotting features of `xarray` is used. For more information about `xarray` see https://docs.xarray.dev/en/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "with h5tbx.H5File() as h5:\n",
    "    dsx = h5.create_dataset('x', data=np.linspace(0, 10, 20), units='mm', long_name='x', make_scale=True)\n",
    "    dsy = h5.create_dataset('y', data=np.linspace(0, 5, 10), units='mm', long_name='y', make_scale=True)\n",
    "    h5.create_dataset('data', data=np.random.random((10, 20)), units='m/s', long_name='velocity', attach_scales=(dsy, dsx))\n",
    "    \n",
    "    # some plotting\n",
    "    plt.figure()\n",
    "    h5['data'][:].plot()\n",
    "    plt.figure()\n",
    "    h5['data'][:].plot.contourf()\n",
    "    plt.figure()\n",
    "    h5['data'][:,0].plot.line(marker='o')\n",
    "    plt.figure()\n",
    "    h5['data'][:].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Other useful features\n",
    "\n",
    "\n",
    "### Creating datasets and groups from a yaml file\n",
    "\n",
    "Sometimes it may be useful to write standard datastes and groups to your file (e.g. always the same attributes for repetative tasks). This can be defined in a yaml file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "dictionary = {'datasets': {'boundary/outlet/y': {'data': 2, 'units': 'm', 'standard_name': 'y_coordinate',\n",
    "                                                         'attrs': {'comment': 'test', 'another_attr': 100.2,\n",
    "                                                                   'array': [1, 2, 3]}}},\n",
    "                      'groups': {'test/grp': {'long_name': 'a test group'}}\n",
    "                      }\n",
    "\n",
    "yaml_filename = h5tbx.generate_temporary_filename(suffix='.yml')\n",
    "with open(yaml_filename, 'w') as f:\n",
    "    yaml.safe_dump(dictionary, f)\n",
    "\n",
    "with h5tbx.H5File() as h5:\n",
    "    h5.from_yaml(yaml_filename)\n",
    "    h5.dump()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
