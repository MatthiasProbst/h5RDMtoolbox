{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Getting started: H5File\n",
    "\n",
    "The package `h5wrapper` adds general and application/domain-specific functionalities to the core interface HDF5 implemented in the package `h5py` (https://docs.h5py.org/en/stable/). This is done by providing so-calle wrapper classes. They\n",
    " - facilitate and streamline the work with HDF5 files and\n",
    " - integrate meta-conventions and file-layout specifications.\n",
    "\n",
    "There are multiple wrapper-classes implemented, each of which exteding functionality and introducing specific layout- and naming-convetions. The basic wrapper-class is called `H5File`. We will first demonstrate the functionalities and concepts around it before the more specialize wrapper classes are shown.\n",
    "\n",
    "The wrapper classes extend but don't limit the funcitonality of the `h5py` package, so everything knwon from it is available also through this package. An HDF file, which is created, read or generally processed with `H5File` requires to respect certain conventions and prinicples. It is possible to ignore them but warnings will be raised. The following sections walk you through the concept and basic functionalities of the first warpper-class `H5File`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5rdmtoolbox as h5tbx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a HDF file\n",
    "\n",
    "There are several ways to open a file with this package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5 = h5tbx.H5File()  # note, not passing `mode` while the filename does not exist: 'r+' is used, otherwise defaultis 'r'\n",
    "print(h5.hdf_filename.name)  # equal to h5.filename but a pathlib.Path and exists also after file is closed\n",
    "h5.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A safer way to work with files is to use python's context manager. This is highly recommended and used throughout the hole documentation and package.\n",
    "\n",
    "Thus, the above cell changes to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File() as h5:\n",
    "    print(h5.hdf_filename.name)  # equal to h5.filename but a pathlib.Path and exists also after file is closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open a file\n",
    "... and displaying (dumping) the content). An interactive html representation of the file content is displayed. At the moment only attributes at the root level exist. They were created when the file was opened in write mode. While `creation_time` and `modification_time` are treated as regular attributes `__h5rdmtoolbox_version__` and `__wrcls__`, thus attributes starting and ending with `__` are special attributes reserved/used by the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File() as h5file:  # default mode is 'r+'\n",
    "    h5file.dump()   # call .sdump() outside of notebooks to get a similar but non-interactive representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `open_wrapper()` opens the file and returns the respective wrapper class instance, namely the one that wrote to the file previously, here `H5File`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.open_wrapper(h5file.hdf_filename) as h5:\n",
    "    h5.dump()\n",
    "    print('Wrapper class instance name: ', type(h5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conventions and layout schema:\n",
    "\n",
    " - Conventions: Mainly naming conventions\n",
    " - Layout (schema): required dataset, data dimension, attributes, groups in an HDF5 file that must always exist\n",
    "\n",
    "\n",
    "Conventions are generally defined by a community (e.g. concept of standard names is used here: [cfconventions.org]) and regulate the usage of certain names. This repository supports the principle set by the climate and forecast community [cfconventions.org]. This means that each dataset must have one of the following or bth attributes:\n",
    "  - `standard_name`\n",
    "  - `long_name`.\n",
    "  \n",
    "While `long_name` is a user-defined and user-readable (as opposed to machine-readable) string without restrictions (e.g. w.r.t length), the attribute `standard_name` cannot be chosen freely but is defined by a `convention`. Each wrapper file (excepet the very basic one `H5Base`) is associated with a convention. A convention can be defined by an `XML` file, read in and passed to the wrapper class. For e.g. `H5Flow` the `FluidConvention` is set per default. Here, for instance, a dataset `x` shall be the created using the wrapper class `H5Flow`, then if the `standard_name` is set it is verified in the associated standard name convention.\n",
    "\n",
    "[cfconventions.org]: http://cfconventions.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File(standard_name_table=None) as h5f:\n",
    "    print(h5f.standard_name_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `H5Flow` class has a *non-empty* convention class set per default. Thus when creating datasets and the parameter `stadnard_name`is passed, it is checked if this name exists in the convention. If so, then the units of the created dataset and the registered convention is verified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5Flow() as h5f:\n",
    "    print(h5f.standard_name_table)\n",
    "    try:\n",
    "        h5f.create_dataset('x', data=1, standard_name='x coordinate', units='m')\n",
    "    except h5tbx.conventions.StandardizedNameError as e:\n",
    "        print(f' > Incorrect standard name: {e}')\n",
    "    try:\n",
    "        h5f.create_dataset('x', data=1, standard_name='x_coordinate', units='kg')\n",
    "    except h5tbx.conventions.StandardizedNameError as e:\n",
    "        print(f' > Incorrect units: {e}')\n",
    "    h5f.create_dataset('x', data=1, standard_name='x_coordinate', units='m')\n",
    "    h5f.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## File creation\n",
    "File creation as used to with `h5py` either with or without defining a filename (when passing no (file)name, write intent is automatically set to 'r+'). If no filename is set, a temporary file is created in a tmp-folder. You may also set the attribute `title` by passing it as parameter during initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File('test.hdf', mode='w') as h5:\n",
    "    h5.dump()\n",
    "    print(h5.layout.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "with h5tbx.H5File() as h5:  # equal to \"with H5File(mode='w') as h5:\"\n",
    "    print(f'HDF5 files initialized without any parameters\\n--> Mode is {h5.mode} and filename is {h5.filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(h5.filenme)\n",
    "except AttributeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h5.hdf_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta convention: Static layouts:\n",
    "\n",
    "Each HDF wrapper class must fulfill certain requirements, e.g.\n",
    " - to have specific attributes\n",
    " - to have specific attributes with a certain value\n",
    " - to have specific groups\n",
    " - to have specific datasets\n",
    " - to have a specific dataset with a specific shape\n",
    "\n",
    "The basic wrapper-class reflects the \"minimum standard\". It must have certain atributes, such as \"creation_time \" or \"title\". To check which one exactly, let's print the content of the file to the screen using `dump()` (outside of jupyter notebooks print the instance or call `sdump()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File(h5tbx.generate_temporary_filename(suffix='.hdf'), 'w') as h5:\n",
    "    h5.attrs['title'] = 'Tutorial data'\n",
    "    h5.attrs['__version__'] = '0.1.12'\n",
    "    test_filename = h5.filename  # or h5.hdf_filename\n",
    "\n",
    "with h5tbx.H5File(test_filename) as h5:\n",
    "    h5.check(silent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset creation\n",
    "The basic dataset creation is no different than with the `h5py` package. However, `H5File` will encourage you to use an attribute `units` and either `long_name` or `standard_name`. You may pass those parameters in the `create_dataset()` function. If not, you will be warned (which you can suppress in the usual way in python `warnings.filterwarnings(\"ignore\")`) but it has no other consequences. You may also directly specify dimension scales during dataset creation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create a simple dataset as usual. It will raise a warning, that no long name or standard name has been passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from h5rdmtoolbox.h5wrapper.h5file import UnitsError\n",
    "with h5tbx.H5File('test.hdf', mode='w', standard_name_table=None) as h5:\n",
    "    try:\n",
    "        h5.create_dataset('temperature', data=np.random.rand(3, 2), long_name='temperature of something')\n",
    "    except h5tbx.conventions.UnitsError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File('test.hdf', mode='w', standard_name_table=None) as h5:\n",
    "    h5.create_dataset('temperature', data=np.random.rand(3, 2), long_name='Surface temperature', units='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File('test.hdf', mode='w', standard_name_table=h5tbx.conventions.FluidStandardNameTable) as h5:\n",
    "    try:\n",
    "        h5.create_dataset('temperature', data=np.random.rand(3, 2), standard_name='Surface temperature', units='')\n",
    "    except h5tbx.conventions.StandardizedNameError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File('test.hdf', mode='w', standard_name_table=h5tbx.conventions.FluidStandardNameTable) as h5:\n",
    "    try:\n",
    "        h5.create_dataset('temperature', data=np.random.rand(3, 2), standard_name='surface temperature', units='')\n",
    "    except h5tbx.conventions.StandardizedNameError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File('test.hdf', mode='w', standard_name_table=h5tbx.conventions.FluidStandardNameTable) as h5:\n",
    "    h5.create_dataset('temperature', data=np.random.rand(3, 2), standard_name='temperature', units='K')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's provide a long_name and a unit, thus no warning will be shown as all requirements are fulfilled. Also note, that we added the parameter `overwrite=True` as we are using the same HDF file and want to overwrite the existing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File('test.hdf', mode='r+', title='tutorial file') as h5:\n",
    "    h5.create_dataset('temperature', data=np.random.rand(3, 2),\n",
    "                     units='degC', overwrite=True, long_name='surface temperature')\n",
    "    print(h5['temperature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better is to pass a standard name. Then the unit is checked for consistency (e.g. temperature has unit kelvin and not meters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File('test.hdf', mode='r+', title='tutorial file') as h5:\n",
    "    try:\n",
    "        h5.create_dataset('temperature', data=np.random.rand(3, 2),\n",
    "                          overwrite=True, standard_name='temperature', units='m')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following temperature dataset has the coorect unit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File('test.hdf', mode='r+', title='tutorial file') as h5:\n",
    "    h5.create_dataset('temperature', data=np.random.rand(3, 2),\n",
    "                     units='degC', overwrite=True, standard_name='temperature')\n",
    "    print(h5['temperature'])\n",
    "    print('---')\n",
    "    h5.sdump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating datasets and groups from a yaml file\n",
    "\n",
    "Sometimes it may be useful to write standard datastes and groups to your file (e.g. always the same attributes for repetative tasks). This can be defined in a yaml file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "dictionary = {'datasets': {'boundary/outlet/y': {'data': 2, 'units': 'm', 'standard_name': 'y_coordinate',\n",
    "                                                         'attrs': {'comment': 'test', 'another_attr': 100.2,\n",
    "                                                                   'array': [1, 2, 3]}}},\n",
    "                      'groups': {'test/grp': {'long_name': 'a test group'}}\n",
    "                      }\n",
    "with open('test.yaml', 'w') as f:\n",
    "    yaml.safe_dump(dictionary, f)\n",
    "\n",
    "with h5tbx.H5File('test.hdf', 'w') as h5:\n",
    "    h5.from_yaml('test.yaml')\n",
    "    h5.dump()\n",
    "    \n",
    "# delete the yaml and hdf file again:\n",
    "Path('test.yaml').unlink()\n",
    "h5.hdf_filename.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Attributes\n",
    "Attributes can be added to the file as known from `h5py`. During file creation some are automatically created, such as package version and file creation/modification time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File(mode='w', title='a test file') as h5:\n",
    "    print(h5.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides adding strings or floats as attributes, also **datasets** or **groups** can be **assigned to an attribute**. Effectively the internal HDF path is stored and when the attributed is requested this is recognized and the respective dataset or group is returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File(mode='w', title='a test file') as h5:\n",
    "    grp = h5.create_group('a group')\n",
    "    h5.attrs['a root group'] = grp\n",
    "    print(h5.attrs['a root group'])\n",
    "    grp.create_dataset('ds', units='m/s', data=[1,2,3], standard_name='x_velocity')\n",
    "    grp.attrs['ref_to_own_ds'] = grp['ds']\n",
    "    d = h5['a group'].attrs['ref_to_own_ds']\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Data exploration and Natural Naming\n",
    "Besides above stated \"rules\", the class gives quick and eays insight in the class by using `.print()` or `.explore()` (the fives an interactive HTML representation only available in notbooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File(mode='w') as h5:\n",
    "    print(h5)\n",
    "    # h5.info()  # equal to print(h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File(mode='w') as h5:\n",
    "    h5.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following raises a warning for not setting `long_name` or `standard_name` and for not defining the `units` of the dataset. Inspection will therefore raise 2 issues (this time `title` was set though):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File(mode='w', title='tutorial test file content') as h5:\n",
    "    h5.create_dataset('test', shape=(2,3), long_name='test dataset', units='')\n",
    "    n = h5.check(silent=False)  # n=1 because title is not set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Natural Naming\" (enable/disable in yaml config see above) allows to address datasets and attributes as if they were attributes of the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File(mode='w', title='tutorial test file content') as h5:\n",
    "    h5.create_dataset('test', shape=(2,3), long_name='test dataset', units='')\n",
    "    ds = h5['test']\n",
    "    ds = h5.test\n",
    "    print(h5.attrs.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interaction with `xarray.DataArray`: Dataset slicing returns a `xarray.DataArray` instead of `np.ndarray`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File(mode='w', title='tutorial test file content') as h5:\n",
    "    h5.create_dataset('test', data=np.random.rand(2,3), long_name='test dataset', units='')\n",
    "    print(type(h5.test[:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Interaction with `xarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to create datasets by passing an `xarray.DataArray`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr =  xr.DataArray(dims=('y', 'x'), data=np.random.rand(3, 2),\n",
    "                                 coords={'y': xr.DataArray(dims='y', data=[1, 2, 3],\n",
    "                                                               attrs={'units': 'm',\n",
    "                                                                      'standard_name': 'y_coordinate'}),\n",
    "                                         'x': xr.DataArray(dims='x',\n",
    "                                                               data=[0, 1],\n",
    "                                                               attrs={'standard_name': 'x_coordinate'})\n",
    "                                        },\n",
    "                                 attrs={'long_name': 'a long name',\n",
    "                                        'units': 'm/s'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the `DataArray` has `units` and `long_name` attributes no warning will be raised in the following lines and no issues are dectected. Coordinates will be also created (if not already exist) as `hdf dimension_scales`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File() as h5:\n",
    "    h5['velocity'] = arr\n",
    "    h5.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and plotting datasets\n",
    "\n",
    "Calling a dataset without slicing returns (as expected) the h5py dataset. However, when slicing an `xarray.DataArray` will be return instead of a `np.ndarray`. With that `xarray.DataArray` quick and easy plotting can be performed. For more information about `xarray` see https://docs.xarray.dev/en/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "with h5tbx.H5File(h5.hdf_filename, mode='r+') as h5:\n",
    "    velocity_h5ds = h5['velocity']  # returns h5py dataset\n",
    "    velocity_xr = h5['velocity'][:]  # slicing returns xarray\n",
    "    \n",
    "    # some plotting\n",
    "    plt.figure()\n",
    "    h5['velocity'][:].plot()\n",
    "    plt.figure()\n",
    "    h5['velocity'][:].plot.contourf()\n",
    "    plt.figure()\n",
    "    h5['velocity'][:,0].plot.line(marker='o')\n",
    "    plt.figure()\n",
    "    h5['velocity'][:].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets, groups and attributes can be addressed by natural naming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File(h5.hdf_filename, mode='r+') as h5:\n",
    "    print(h5.creation_time)\n",
    "    print(h5.attrs)\n",
    "    print(h5.velocity)\n",
    "    h5.create_group('test_group', long_name='a test group')\n",
    "    print(h5.test_group)\n",
    "    print('\\n')\n",
    "    h5.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File(h5.hdf_filename, 'r') as h5:\n",
    "    print(h5['x'])\n",
    "    vel = h5['velocity'][:,:]\n",
    "    print(vel)\n",
    "    x=h5['x'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
