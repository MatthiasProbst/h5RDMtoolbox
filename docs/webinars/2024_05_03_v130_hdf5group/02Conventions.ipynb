{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7e60170-28a1-49ff-a53f-89a9f4a5d764",
   "metadata": {},
   "source": [
    "# 02 Conventions\n",
    "\n",
    "Conventions define \"standard attributes\", which are expected during dataset, group or file creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3801c6c3-c31c-484c-825e-c11c530de418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5rdmtoolbox as h5tbx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ea705-b8a3-4413-830e-9c81b552c199",
   "metadata": {},
   "source": [
    "The are defined in YAML files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcc12582-bf10-4731-9d26-b64fe68ff5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = h5tbx.convention.Convention.from_yaml('webinar_convention.yaml', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4305b791-fed1-46cf-a51a-c5ec1d0aa100",
   "metadata": {},
   "source": [
    "Register the convention on this computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "367f57e3-14df-4405-9907-966db751803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.register()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1725e9-c1b5-409d-b8ac-85b903bf0fb7",
   "metadata": {},
   "source": [
    "Enable to make effective in this session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1ed55c1-3a9e-4c68-bba0-59b5b1d5ef57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "using(\"webinar_convention\")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5tbx.use('webinar_convention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a5e7892-2c74-4a59-822a-a81ed40c0105",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.File(creation_mode='numerical') as h5:\n",
    "    h5.create_dataset('test', data=3.4, units='m/s')\n",
    "    h5.create_group('grp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c07f2c9-ed2f-4387-926b-f31652917b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "using(\"h5py\")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5tbx.use(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6c78769-cfa1-4657-8477-bcae222ccfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method create_dataset in module h5rdmtoolbox.wrapper.core:\n",
      "\n",
      "create_dataset(name, shape=None, dtype=None, data=None, overwrite=None, chunks=True, make_scale=False, attach_data_scale=None, attach_data_offset=None, attach_scales=None, ancillary_datasets=None, attrs=None, **kwargs) -> h5rdmtoolbox.protocols.H5TbxDataset method of h5rdmtoolbox.wrapper.core.File instance\n",
      "    Creating a dataset. Allows attaching/making scale, overwriting and setting attributes simultaneously.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    name : str\n",
      "        Name of dataset\n",
      "    shape : tuple, optional\n",
      "        Dataset shape. see h5py doc. Default None. Required if data=None.\n",
      "    dtype : str, optional\n",
      "        dtype of dataset. see h5py doc. Default is dtype('f')\n",
      "    data : numpy ndarray, default=None\n",
      "        Provide data to initialize the dataset.  If not used,\n",
      "        provide shape and optionally dtype via kwargs (see more in\n",
      "        h5py documentation regarding arguments for create_dataset\n",
      "    overwrite : bool, default=None\n",
      "        If the dataset does not already exist, the new dataset is written and this parameter has no effect.\n",
      "        If the dataset exists and ...\n",
      "        - ... overwrite is None, then h5py behaviour is enabled meaning that if a dataset exists h5py will raise an error\n",
      "        - ... overwrite is True, then dataset is deleted and rewritten according to method parameters\n",
      "        - ... overwrite is False, then dataset creation has no effect. Existing dataset is returned.\n",
      "    chunks : bool or according to h5py.File.create_dataset documentation\n",
      "        Needs to be True if later resizing is planned\n",
      "    make_scale: bool, default=False\n",
      "        Makes this dataset scale. The parameter attach_scale must be uses, thus be None.\n",
      "    attach_data_scale: Union[None, h5py.Dataset], default=None\n",
      "        If not None, attach this dataset as scale to the dataset.\n",
      "    attach_data_offset: Union[None, h5py.Dataset], default=None\n",
      "        If not None, attach this dataset as offset to the dataset.\n",
      "    attach_scales : tuple, optional\n",
      "        Tuple defining the datasets to attach scales to. Content of tuples are\n",
      "        internal hdf paths. If an axis should not be attached to any axis leave it\n",
      "        empty (''). Default is ('',) which attaches no scales\n",
      "        Note: internal hdf5 path is relative w.r.t. this dataset, so be careful\n",
      "        where to create the dataset and to which to attach the scales!\n",
      "        Also note, that if data is a xr.DataArray and attach_scales is not None,\n",
      "        coordinates of xr.DataArray are ignored and only attach_scales is\n",
      "        considered.\n",
      "    ancillary_datasets: Union[None, Dict[h5py.Dataset]], optional=None\n",
      "        If not None, attach flags to dataset. If str, it is interpreted as\n",
      "        internal hdf path. If h5py.Dataset, it is interpreted as dataset to attach\n",
      "        flags to. Default is None, which means no flags are attached. If a flag\n",
      "        dataset is attached the return object is a xr.Dataset object, which additionally\n",
      "        includes the flag data array.\n",
      "    attrs : dict, optional\n",
      "        Allows to set attributes directly after dataset creation. Default is\n",
      "        None, which is an empty dict\n",
      "    **kwargs : dict, optional\n",
      "        Dictionary of standard arguments and other keyword arguments that are passed\n",
      "        to the parent function.\n",
      "        For **kwargs, see h5py.File.create_dataset.\n",
      "    \n",
      "        Standard arguments are defined by a convention and hence expected keywords\n",
      "        depend on the registered standard attributes.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    ds : h5py.Dataset\n",
      "        created dataset\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "with h5tbx.File() as h5:\n",
    "    print(help(h5.create_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53d10324-68c6-4eb6-b970-9e945a284ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "using(\"h5py\")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5tbx.use(None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
