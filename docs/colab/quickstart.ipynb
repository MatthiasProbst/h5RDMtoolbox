{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install h5rdmtoolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Planning\n",
    "\n",
    "- Decide to use HDF5 as your core file format\n",
    "- Define important attributes and their usage in a metadata convention (e.g. a YAML file)\n",
    "- Publish your convention on a repository like [Zenodo](https://zenodo.org/)\n",
    "\n",
    "At this time we assume, that we have done this already, thus we'll be using a convention published on [Zenodo (doi/rec-id: 10428822)](https://zenodo.org/records/10428822), that already exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5rdmtoolbox as h5tbx\n",
    "from h5rdmtoolbox.repository import zenodo\n",
    "from h5rdmtoolbox.tutorial import C\n",
    "\n",
    "print(h5tbx.__version__)\n",
    "\n",
    "zenodo_repo = zenodo.ZenodoRecord(10428822)\n",
    "cv = h5tbx.convention.from_repo(zenodo_repo, 'tutorial_convention.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important step is defining the expected layout of a file. For this simple example, we \"only\" want to require users to provide the attribute \"units\" with every dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5rdmtoolbox import layout\n",
    "from h5rdmtoolbox import database\n",
    "\n",
    "lay = layout.Layout()\n",
    "spec_has_units = lay.add(\n",
    "    database.FileDB.find,\n",
    "    flt={'units': {'$exists': True}},\n",
    "    objfilter='dataset'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Collecting\n",
    "\n",
    "We start with writing data to an HDF5 file. This is syntactically almost like using `h5py`, just with a few features wrapped around it.\n",
    "\n",
    "- Fill an HDF5 file with the required data and mandatory metadata\n",
    "- Data may come in various sources, e.g. from a measurement, a simulation or a database\n",
    "- HDF5 is best for multidimensional data, but can also be used for 1D data\n",
    "- When writing the HDF5 files, the convention is automatically validating the metadata input, which are the attributes\n",
    "  or the datasets and groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start writing data to the file, we must enable the convention. This results in changing the behaviour of methods like `create_datasets` as they now require parameters like `units` for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable the convention:\n",
    "h5tbx.use(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'my_file.hdf'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with h5tbx.File(filename,\n",
    "                mode='w',\n",
    "                data_type='experimental',\n",
    "                contact='https://orcid.org/0000-0001-8729-0482') as h5:\n",
    "    # create a dataset\n",
    "    h5.create_dataset(name='time', data=np.linspace(0, 1, 1000),\n",
    "                      standard_name='time', units='ms', make_scale=True)\n",
    "    h5.create_dataset(name='u',\n",
    "                      data=np.random.normal(10, 2, 1000),\n",
    "                      standard_name='x_velocity',\n",
    "                      units='m/s',\n",
    "                      attach_scale='time')\n",
    "    h5['u'].dims[0].attach_scale(h5['time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining the metadata\n",
    "\n",
    "We already provided quite some metadata with the file. Although, attributes like \"units\" is quite self-explaining, let's associate it with a persistent identifier. In this way, the metadata becomes *FAIR*. More on this can be found [here](https://h5rdmtoolbox.readthedocs.io/en/latest/userguide/wrapper/FAIRAttributes.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ontolutils import M4I, QUDT_UNIT\n",
    "\n",
    "with h5tbx.File(filename, 'r+') as h5:\n",
    "    h5.u.iri['units'].predicate = 'http://qudt.org/schema/qudt/Unit'\n",
    "    h5.u.iri['units'].object = QUDT_UNIT.M_PER_SEC\n",
    "    h5.iri['contact'].predicate = M4I.orcidId\n",
    "\n",
    "    h5.rdf['data_type'].definition = 'The source type of the data'  # define the meaning of the attribute with a custom text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will see one practical effect of assigning IRIs to the metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analyzing\n",
    "\n",
    "Let's open the file, inspect its content and plot a dataset.\n",
    "\n",
    "## 3.1 HDF content dump\n",
    "Use the *dump()* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.File(filename, mode='r') as h5:\n",
    "    # inspect the file layout and content:\n",
    "    h5.dump(collapsed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or extract metadata as a JSON-LD file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5rdmtoolbox import jsonld\n",
    "print(jsonld.dumps(filename, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Plotting\n",
    "\n",
    "First slice the dataset. We will receive a `xr.DataArray`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.File(filename, mode='r') as h5:\n",
    "    u = h5.u[:]\n",
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot it using the conventional way by using `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "u.plot(marker='.', label='raw')\n",
    "u.rolling(time=10).mean().plot(label='rolling mean (win=10)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or do everything in just a few lines (this time with a histogram) thanks to the `xrarray` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.File(filename, mode='r') as h5:\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    h5.u[:].plot.hist(bins=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sharing your data on a repository\n",
    "\n",
    "Sharing data is important. We want to upload our HDF5 file to Zenodo for long-time storage.\n",
    "\n",
    "## 4.1 Validation\n",
    "Before doing so, let's first check if the file fulfills the requirements, which we defined in the layout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = lay.validate(filename)\n",
    "res.is_valid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes it does. We can upload it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Upload\n",
    "Currently, only the interface class to [Zenodo](https://zenodo.org/) is implemented. It is a popular choice to publish open access data. To create and upload data, you must generate an API token. If you don't have it, skip the next step or request one from Zenodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5rdmtoolbox.repository import zenodo\n",
    "from h5rdmtoolbox import dump_jsonld\n",
    "\n",
    "TOKEN = None  # replace with your token\n",
    "\n",
    "if TOKEN:\n",
    "    zenodo.set_api_token(TOKEN)\n",
    "    repo = zenodo.ZenodoSandboxDeposit(None)  # create a new one are provide the record ID.\n",
    "    # The method \"upload_hdf_file\" requires a mapper function which creates an additional file which only contains metadata of the file.\n",
    "    # If the HDF5 file is verly large, the user can first download the metadata file (in our case a json file) and inspect the metadata before\n",
    "    # downloading the full file\n",
    "    repo.upload_hdf_file(filename, metamapper=dump_jsonld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Reusing\n",
    "\n",
    " - Find the data by searching through a file director\n",
    " - The root folder is the parent of the created file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdb = h5tbx.database.FileDB(filename)\n",
    "fdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find exactly one file with \"standard_name=x_velocity\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = fdb.find_one({'standard_name': {'$eq': 'x_velocity'}})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable \"result\" is the found dataset (class is `LDataset`, a wrapper around a closed HDF dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[:].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
