{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "897c6a94-d44e-4b89-bcb2-56a913faaafb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Quick Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe30cac8-1a28-46e1-b886-8b10979c4098",
   "metadata": {},
   "source": [
    "This chapter gives a quick overview into how to use the package. Detailed explanations can be found in the subchapters of the respective sub-classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d38b11-9866-4d04-ae68-2354cf992ac1",
   "metadata": {},
   "source": [
    "Import the package and give it an alias, e.g. `h5tbx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d821ed-27db-46eb-9206-435ac5208f53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import h5rdmtoolbox as h5tbx\n",
    "from h5rdmtoolbox import tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26114e99-7d0e-4b6f-8b41-7c6e8e76b78b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Select a convention\n",
    "The file content is controlled by means of a `convention`. This means that specific attributes are required for HDF groups or datasets.\n",
    "\n",
    "They can be understood as rules, which are validated during usage. To make those rules to become effective, the convention must be imported and enabled. Conventions can be created by the user, too. More on this [here](../conventions/standard_attributes_and_conventions.ipynb).\n",
    "\n",
    "For now, we select the existing one, which is published on [Zenodo](https://zenodo.org/record/8276716)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d57bb0fe-9187-4350-9c1e-1fe117613dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\da4323\\Documents\\programming\\GitHub\\h5RDMtoolbox\\h5rdmtoolbox\\conventions\\core.py:70: UserWarning: The scale-offset feature is implemented as user-defined dataset decoders. The parameter `use_scale_offset` is ignored here!.\n",
      "  warnings.warn('The scale-offset feature is implemented as user-defined dataset decoders. '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1mConvention(\"h5rdmtoolbox-tutorial-convention\")\u001b[0m\n",
       "contact: https://orcid.org/0000-0001-8729-0482\n",
       "  File.__init__():\n",
       "    * \u001b[1mdata_type\u001b[0m:\n",
       "\t\tType of data in file. Can be numerical, analytical or experimental.\n",
       "    * \u001b[1mcontact\u001b[0m:\n",
       "\t\tContact or responsible person for the full file. Contact is represented by an ORCID.\n",
       "    * \u001b[3mstandard_name_table\u001b[0m (default=<h5rdmtoolbox.conventions.consts.DefaultValue object at 0x000002C531E9CAF0>):\n",
       "\t\tThe standard name table of the convention.\n",
       "    * \u001b[3mcomment\u001b[0m (default=None):\n",
       "\t\tComment describes the file content in more detail.\n",
       "    * \u001b[3mreferences\u001b[0m (default=None):\n",
       "\t\tWeb resources servering as references for the full file.\n",
       "  Group.create_dataset():\n",
       "    * \u001b[1munits\u001b[0m:\n",
       "\t\tThe physical unit of the dataset. If dimensionless, the unit is ''.\n",
       "    * \u001b[1mstandard_name\u001b[0m:\n",
       "\t\tStandard name of the dataset. If not set, the long_name attribute must be given.\n",
       "    * \u001b[1mlong_name\u001b[0m:\n",
       "\t\tAn comprehensive description of the dataset. If not set, the standard_name attribute must be given.\n",
       "    * \u001b[3mcomment\u001b[0m (default=None):\n",
       "\t\tComment describes the dataset in more detail.\n",
       "    * \u001b[3mreferencesdataset\u001b[0m (default=None):\n",
       "\t\tWeb resources servering as references for the dataset.\n",
       "  Group.create_group():\n",
       "    * \u001b[3mcomment\u001b[0m (default=None):\n",
       "\t\tComment describes the group content in more detail.\n",
       "    * \u001b[3mreferences\u001b[0m (default=None):\n",
       "\t\tWeb resources servering as references for the group."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = h5tbx.conventions.from_zenodo('8296801')\n",
    "# cv = h5tbx.conventions.from_yaml(tutorial.get_standard_attribute_yaml_filename())\n",
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a073571b-0284-487f-9758-4bff9e9891d6",
   "metadata": {},
   "source": [
    "From the above representation string of the convention object we can read which attributes are *optional* or **required** for file creation (`__init__`), dataset creation (`create_dataset`) or group creation (`create_group`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c3c0f6-4a1d-452a-85c6-1731a42b0bf4",
   "metadata": {},
   "source": [
    "Without enabling the convention, the working with HDF5 files through the `h5rdmtoolbox` is almost (we got a few additional features which make life a bit easier) as by using `h5py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbfd20b3-62c7-4c00-bcec-76b937c9fad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<head><style>/*\r\n",
       "CSS inspired by xarray: https://github.com/pydata/xarray\r\n",
       "*/\r\n",
       ".h5tb-header > div,\r\n",
       ".h5tb-header > ul {\r\n",
       "    display: inline;\r\n",
       "    margin-top: 0;\r\n",
       "    margin-bottom: 0;\r\n",
       "}\r\n",
       "\r\n",
       ".h5tb-dataarray-cls,\r\n",
       ".h5tb-dataarray-name {\r\n",
       "    margin-left: 2px;\r\n",
       "    margin-right: 10px;\r\n",
       "}\r\n",
       "\r\n",
       ".h5tb-dataarray-name {\r\n",
       "    color: #000;\r\n",
       "}\r\n",
       "\r\n",
       ".h5grp-sections {\r\n",
       "    list-style: none;\r\n",
       "    padding: 3px;\r\n",
       "    margin: 0;\r\n",
       "}\r\n",
       "\r\n",
       ".h5grp-sections input {\r\n",
       "    display: none;\r\n",
       "}\r\n",
       "\r\n",
       ".h5grp-sections label {\r\n",
       "    display: inline;\r\n",
       "}\r\n",
       "\r\n",
       ".h5grp-sections > li > input + label > span {\r\n",
       "    display: inline;\r\n",
       "    margin-left: 4px;\r\n",
       "}\r\n",
       "\r\n",
       ".h5grp-sections > li > input:checked + label > span {\r\n",
       "    display: none;\r\n",
       "}\r\n",
       "\r\n",
       ".h5grp-sections input:enabled + label {\r\n",
       "    cursor: pointer;\r\n",
       "}\r\n",
       "\r\n",
       ".h5grp-sections input:not(.h5tb-values-in) ~ ul {\r\n",
       "    display: none;\r\n",
       "}\r\n",
       "\r\n",
       ".h5grp-sections input:not(.h5tb-values-in):checked ~ ul {\r\n",
       "    display: block;\r\n",
       "}\r\n",
       "\r\n",
       ".h5grp-sections > li > input + label {\r\n",
       "    width: 140px;\r\n",
       "    color: #555;\r\n",
       "    font-weight: 500;\r\n",
       "    padding: 4px 0 2px 0;\r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       ".h5grp-sections > li > input + label:before {\r\n",
       "    display: inline-block;\r\n",
       "    content: '+';\r\n",
       "    font-size: 11px;\r\n",
       "    width: 15px;\r\n",
       "    text-align: center;\r\n",
       "}\r\n",
       "\r\n",
       ".h5grp-sections > li > input:disabled + label:before {\r\n",
       "    color: #777;\r\n",
       "}\r\n",
       "\r\n",
       ".h5grp-sections > li > input:checked + label:before {\r\n",
       "    content: '-';\r\n",
       "}\r\n",
       "\r\n",
       ".h5tb-dim-list {\r\n",
       "    display: inline-block !important;\r\n",
       "    list-style: none;\r\n",
       "    padding: 0;\r\n",
       "}\r\n",
       "\r\n",
       ".h5tb-dim-list li {\r\n",
       "    display: inline-block;\r\n",
       "    padding: 0;\r\n",
       "    margin: 0;\r\n",
       "}\r\n",
       "\r\n",
       ".h5tb-dim-list:before {\r\n",
       "    content: '(';\r\n",
       "}\r\n",
       "\r\n",
       ".h5tb-dim-list:after {\r\n",
       "    content: ')';\r\n",
       "}\r\n",
       "\r\n",
       ".h5tb-dim-list li:not(:last-child):after {\r\n",
       "    content: ',';\r\n",
       "}\r\n",
       "\r\n",
       ".h5tb-has-index {\r\n",
       "    text-decoration: underline;\r\n",
       "}\r\n",
       "\r\n",
       ".h5tb-var-list {\r\n",
       "    list-style: none;\r\n",
       "    padding: 0;\r\n",
       "    margin: 0;\r\n",
       "}\r\n",
       "\r\n",
       ".h5tb-var-list > li {\r\n",
       "    background-color: #fcfcfc;\r\n",
       "    overflow: hidden;\r\n",
       "}\r\n",
       "\r\n",
       ".h5tb-var-list > li:nth-child(odd) {\r\n",
       "    background-color: #efefef;\r\n",
       "}\r\n",
       "\r\n",
       ".h5tb-var-list li:hover {\r\n",
       "    background-color: rgba(3, 169, 244, .2);\r\n",
       "}\r\n",
       "\r\n",
       ".h5tb-var-list li > span {\r\n",
       "    display: inline-block;\r\n",
       "}\r\n",
       "\r\n",
       "input.h5tb-varname-in + label {\r\n",
       "    width: 140px;\r\n",
       "    padding-left: 0;\r\n",
       "    font-weight: bold;\r\n",
       "}\r\n",
       "\r\n",
       ".h5tb-dataset {\r\n",
       "    width: 100px;\r\n",
       "}\r\n",
       "\r\n",
       ".h5tb-attributevalue {\r\n",
       "    width: 100px;\r\n",
       "    text-align: left;\r\n",
       "    color: #888;\r\n",
       "    white-space: nowrap;\r\n",
       "    font-size: 12px;\r\n",
       "}\r\n",
       "\r\n",
       "input.h5tb-varname-in + label:before {\r\n",
       "    content: ' ';\r\n",
       "    display: inline-block;\r\n",
       "    font-size: 11px;\r\n",
       "    width: 15px;\r\n",
       "    padding-left: 2px;\r\n",
       "    padding-right: 2px;\r\n",
       "    text-align: center;\r\n",
       "    color: #aaa;\r\n",
       "    text-decoration: none !important;\r\n",
       "}\r\n",
       "\r\n",
       "input.h5tb-varname-in:enabled + label:hover:before {\r\n",
       "    color: #000;\r\n",
       "}\r\n",
       "\r\n",
       "input.h5tb-varname-in:checked + label:before {\r\n",
       "    color: #ccc;\r\n",
       "}\r\n",
       "\r\n",
       ".h5tb-dims {\r\n",
       "    width: 280px;\r\n",
       "    white-space: nowrap;\r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       ".h5tb-unit {\r\n",
       "    width: 96px;\r\n",
       "    white-space: nowrap;\r\n",
       "    padding-right: 2px;\r\n",
       "    text-align: right;\r\n",
       "    color: #555;\r\n",
       "    font-style: italic;\r\n",
       "}\r\n",
       "\r\n",
       ".h5tb-attr-list {\r\n",
       "    list-style: none;\r\n",
       "    background-color: #fff;\r\n",
       "    padding-bottom: 6px;\r\n",
       "    color: #555;\r\n",
       "}\r\n",
       "\r\n",
       ".h5tb-attr-list li,\r\n",
       ".h5tb-attr-list li:hover {\r\n",
       "    background-color: #fff;\r\n",
       "}\r\n",
       "\r\n",
       ".h5tb-dim-highlight {\r\n",
       "    background-color: rgba(3, 169, 244, .2);\r\n",
       "    cursor: pointer;\r\n",
       "}</style></head>\n",
       "<div class='h5tb-warp'>\n",
       "\n",
       "              <ul style=\"list-style-type: none;\" class=\"h5grp-sections\">\n",
       "                    <li>\n",
       "                        <input id=\"group-ds--46696118900\" type=\"checkbox\" checked>\n",
       "                        <label style=\"font-weight: bold\" for=\"group-ds--46696118900\">\n",
       "                        /<span>(0)</span></label>\n",
       "                  \n",
       "\n",
       "                    <ul class=\"h5tb-attr-list\"><li style=\"list-style-type: none; font-style: italic\">__h5rdmtoolbox_version__ : 0.8rc1</li>\n",
       "                    </ul>\n",
       "</li>\n",
       "</ul>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with h5tbx.File() as h5:\n",
    "    h5.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa06687-c9c0-4a20-af30-91f337fb09d5",
   "metadata": {},
   "source": [
    "##### Now, we enable the convention ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1bb2a60-752c-4d14-844e-16841a060d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "using(\"h5rdmtoolbox-tutorial-convention\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5tbx.use(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d071903e-e60c-45c6-a4e5-758533975171",
   "metadata": {},
   "source": [
    "... and get an error, because we are not providing a \"data_type\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5049224-a978-4baf-ada6-70b8ede3dde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The standard attribute \"data_type\" is required but not provided.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with h5tbx.File() as h5:\n",
    "        pass\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c93e7-6d5f-4187-896e-f8223d0449c3",
   "metadata": {},
   "source": [
    "## Difference to `h5py` package\n",
    "\n",
    "The `h5RDMtoolbox` is built upon the `h5py` package. Besides the ability to include conventions, more convenient features are implemented.\n",
    "\n",
    "### Filename\n",
    "We could already see, that we did not have to provide a filename when creating a new file. If none is provided, a temporary file is created. Also, `hdf_filename` is provided as an additional property allowing to work with the filename even after the file has been closed *and* to work with `pathlib.Path` objects instead of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "641d270b-6c02-4296-8ec7-34dc05af90df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tmp2.hdf'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with h5tbx.use(None):\n",
    "    with h5tbx.File() as h5:\n",
    "        pass\n",
    "h5.hdf_filename.name  # equal to h5.filename but a pathlib.Path and exists also after the file is closed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4228862d-8150-40fb-af60-0d46625dcada",
   "metadata": {},
   "source": [
    "## Datasets/xarray interface\n",
    "\n",
    "With the `h5rdmtoolbox` you receive a `xr.DataArray` object instead of a `np.ndarray` when an HDF5 dataset is sliced. Thus, meta information (the attributes of the dataset) is still provided with the data and useful features like plotting is possible.\n",
    "\n",
    "Let's create some sample data and inspect the data afterward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69936b5e-d89b-47e5-868c-2121f55b264f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "make_new_dset() got an unexpected keyword argument 'offset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m\n\u001b[0;32m      7\u001b[0m vel_hdf_filename \u001b[38;5;241m=\u001b[39m h5\u001b[38;5;241m.\u001b[39mhdf_filename \u001b[38;5;66;03m# store for later use\u001b[39;00m\n\u001b[0;32m      9\u001b[0m ds_time \u001b[38;5;241m=\u001b[39m h5\u001b[38;5;241m.\u001b[39mcreate_dataset(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     10\u001b[0m                             data\u001b[38;5;241m=\u001b[39mtime, \n\u001b[0;32m     11\u001b[0m                             units\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m                             long_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeasurement time\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m                             make_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 15\u001b[0m ds_signal \u001b[38;5;241m=\u001b[39m \u001b[43mh5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                              \u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mV\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                              \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2.5 m/s/V\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mlong_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mair velocity in pipe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mattach_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_time\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\h5tbx\\lib\\site-packages\\forge\\_revision.py:328\u001b[0m, in \u001b[0;36mRevision.__call__.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(\u001b[38;5;28mcallable\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;66;03m# pylint: disable=E1102, not-callable\u001b[39;00m\n\u001b[0;32m    327\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m inner\u001b[38;5;241m.\u001b[39m__mapper__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmapped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmapped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\programming\\GitHub\\h5RDMtoolbox\\h5rdmtoolbox\\wrapper\\core.py:732\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[1;34m(self, name, shape, dtype, data, overwrite, chunks, make_scale, attach_scales, ancillary_datasets, attrs, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m         _ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate_dataset(name,\n\u001b[0;32m    726\u001b[0m                                      shape\u001b[38;5;241m=\u001b[39mshape,\n\u001b[0;32m    727\u001b[0m                                      dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    728\u001b[0m                                      data\u001b[38;5;241m=\u001b[39m_data,\n\u001b[0;32m    729\u001b[0m                                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m         \u001b[38;5;66;03m# create ND dataset with shape, data is assigned later\u001b[39;00m\n\u001b[1;32m--> 732\u001b[0m         _ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mcompression_opts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_opts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m                                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    741\u001b[0m     \u001b[38;5;66;03m# no data given, initialize with shape only\u001b[39;00m\n\u001b[0;32m    742\u001b[0m     _ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate_dataset(name, shape\u001b[38;5;241m=\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mdtype, data\u001b[38;5;241m=\u001b[39m_data,\n\u001b[0;32m    743\u001b[0m                                  compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    744\u001b[0m                                  compression_opts\u001b[38;5;241m=\u001b[39mcompression_opts,\n\u001b[0;32m    745\u001b[0m                                  chunks\u001b[38;5;241m=\u001b[39mchunks,\n\u001b[0;32m    746\u001b[0m                                  \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\h5tbx\\lib\\site-packages\\h5py\\_hl\\group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[1;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[0;32m    180\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    181\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[1;32m--> 183\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "\u001b[1;31mTypeError\u001b[0m: make_new_dset() got an unexpected keyword argument 'offset'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "time = np.linspace(0, np.pi/4, 21) # units [s]\n",
    "signal = np.sin(2*np.pi*3*time) # units [V], physical: [m/s]\n",
    "\n",
    "with h5tbx.File(contact=h5tbx.__author_orcid__, data_type='experimental') as h5:\n",
    "    vel_hdf_filename = h5.hdf_filename # store for later use\n",
    "    \n",
    "    ds_time = h5.create_dataset(name='time',\n",
    "                                data=time, \n",
    "                                units='s',\n",
    "                                long_name='measurement time',\n",
    "                                make_scale=True)\n",
    "    \n",
    "    ds_signal = h5.create_dataset(name='vel',\n",
    "                                  data=signal,\n",
    "                                  units='V',\n",
    "                                  offset=10.0,\n",
    "                                  scale='2.5 m/s/V',\n",
    "                                  long_name='air velocity in pipe',\n",
    "                                  attach_scale=ds_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b486d-4adc-47ce-89ac-42ff6c2f3d41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5tbx.File(vel_hdf_filename) as h5:\n",
    "    vel_data = h5['vel'][:]\n",
    "    vel_data.plot(marker='o')\n",
    "    \n",
    "vel_data  # this returns the interactive view of the array and its meta data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1627e47a-37d1-45b2-8adc-d04d038b1574",
   "metadata": {},
   "source": [
    "## Natural Naming\n",
    "Until here we used the conventional way of addressing variables and groups in a dictionary-like style. `h5RDMtoolbox` allows using \"natural naming\" which means that we can address those objects as if they were attributes. Make sure `h5tbx.config.natural_naming` is set to `True` (the default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b678a5-403f-4055-9d1b-45153dcb7b58",
   "metadata": {},
   "source": [
    "Let's first disable `natural_naming`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "224c1b74-bd60-4c2e-b498-1f3dab56aa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File' object has no attribute 'vel'\n"
     ]
    }
   ],
   "source": [
    "with h5tbx.set_config(natural_naming=False):\n",
    "    with h5tbx.File(vel_hdf_filename, 'r') as h5:\n",
    "        try:\n",
    "            ds = h5.vel[:]\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3514ae51-07ca-4e97-a37f-bbd3a8c29f3d",
   "metadata": {},
   "source": [
    "Enable it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a088543-0338-4022-a691-4108a95b1951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5tbx.set_config(natural_naming=True):\n",
    "    with h5tbx.File(vel_hdf_filename, 'r') as h5:\n",
    "        ds = h5.vel[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcc735d-8422-4710-a794-984a59580ad6",
   "metadata": {},
   "source": [
    "## Inspect file content\n",
    "Often it is necessary to inspect the content of a file (structure, metadata, not the raw data). Calling `dump()` on a group represents the content (dataset, groups and attributes) as a pretty and interactive (!) HTML representation. This is adopted from the `xarray` package. All credits for this idea go there. The representation here avoids showing data, though. Outside an IPython environment, call `sdump()` to get a string representation of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6775f0-f3f0-40ac-887d-d00068188138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5tbx.File(vel_hdf_filename) as h5:\n",
    "    h5.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f5ebb1-2587-41f1-a9eb-1f52b9ba70de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5tbx.File(vel_hdf_filename) as h5:\n",
    "    h5.sdump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b88c46-3640-46fa-86f1-4a2b92fef688",
   "metadata": {},
   "source": [
    "### Layouts\n",
    "\n",
    "Layouts define how a file is expected to be organized, which groups and datasets must exist, which attributes are expected and much more. Layout define expectations and thus help file exchange where multiple users are involved. E.g. for numerical and experimental data, layouts are defined such that the minimum data will exist. If a layout validation exists, the exchanged file is rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12769f-f6d8-40d6-b352-01844767d0f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from h5rdmtoolbox.conventions.layout import Validator\n",
    "import datetime\n",
    "\n",
    "class ValidCreationTime(Validator):\n",
    "    \n",
    "    def __init__(self, optional):\n",
    "        super().__init__(reference=None, optional=optional)\n",
    "    \n",
    "    def validate(self, date_string):\n",
    "        try:\n",
    "            dt = datetime.datetime.strptime(date_string, '%Y-%m-%d %H:%M:%S.%f')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bcb52f-c8f4-4ca3-8929-0dce1fb71d8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from h5rdmtoolbox.conventions.layout import Layout, validators\n",
    "\n",
    "lay = h5tbx.conventions.layout.Layout()\n",
    "lay['/'].attrs['creation_time'] = ValidCreationTime(False)\n",
    "lay['/'].attrs['title'] = validators.ValidString(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5794b7-c102-49e1-8f11-508916855ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.use(None): # switch off the convention for now\n",
    "    with h5tbx.File() as h5:\n",
    "        lay.validate(h5)\n",
    "lay.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a48df6-f238-4ce5-8b03-20b49afe75de",
   "metadata": {},
   "outputs": [],
   "source": [
    "lay.get_failed_validations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b14e67-25f9-4b51-8e9f-7af4702b0fd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5tbx.use(None): # switch off the convention for now\n",
    "    with h5tbx.File() as h5:    \n",
    "        h5.attrs['creation_time'] = str(datetime.datetime.now())\n",
    "        lay.validate(h5)\n",
    "lay.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d5f216-121e-4eb5-b368-e49de2637cba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lay.get_failed_validations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8329f5f0-c483-46ee-bbeb-0d92d9e8d405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5tbx.use(None): # switch off the convention for now\n",
    "    with h5tbx.File() as h5:\n",
    "        h5.attrs['creation_time'] = str(datetime.datetime.now())\n",
    "        h5.attrs['title'] = 'Test file'\n",
    "        lay.validate(h5)\n",
    "lay.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1482a-e59c-4986-a2c6-1860d1003358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19e9685-0ec8-43eb-aa47-db1303d003c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
