{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "897c6a94-d44e-4b89-bcb2-56a913faaafb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Quick Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe30cac8-1a28-46e1-b886-8b10979c4098",
   "metadata": {},
   "source": [
    "This chapter gives a quick overview into how to use the package. Detailed explanations can be found in the subchapters of the respective sub-classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d38b11-9866-4d04-ae68-2354cf992ac1",
   "metadata": {},
   "source": [
    "Import the package and give it an alias, e.g. `h5tbx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d821ed-27db-46eb-9206-435ac5208f53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import h5rdmtoolbox as h5tbx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26114e99-7d0e-4b6f-8b41-7c6e8e76b78b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Select a convention\n",
    "The file content is controlled by means of a `convention`, which is a set of standardized attributes. They enforce the user to provide certain meta-data and at the same controls its value (e.g. syntax). Either use the pre-defined convention (`tbx`) or [create your own](../conventions/standard_attributes.ipynb). For now, we select the existing one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57bb0fe-9187-4350-9c1e-1fe117613dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "h5tbx.use('tbx')\n",
    "h5tbx.get_current_convention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a073571b-0284-487f-9758-4bff9e9891d6",
   "metadata": {},
   "source": [
    "From the representation string of the convention object we can read which attributes are *optional* or **required** for file creation (`__init__`), dataset creation (`create_dataset`) or group creation (`create_group`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c93e7-6d5f-4187-896e-f8223d0449c3",
   "metadata": {},
   "source": [
    "## Create an HDF file\n",
    "\n",
    "We recommend using python's context manager (`with` ...). it is not required to provide a filename. If so, a **temporary file** is created and deleted after the session, thus perfectly suited for this tutorial session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641d270b-6c02-4296-8ec7-34dc05af90df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5tbx.File(title='A test file') as h5:\n",
    "    print(h5.hdf_filename.name)  # equal to h5.filename but a pathlib.Path and exists also after the file is closed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ec3475-ae2f-47b6-a515-da57c9af52b1",
   "metadata": {},
   "source": [
    "## Create a dataset\n",
    "\n",
    "To create a dataset we need to call `create_dataset`. We already know what, that by enabling the \"tbx\"-convention a few additional parameters can or must be passed. By enabling or disabling a convention, the method signature is always updated acordingly, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2963b31c-1f7b-4183-911b-74d88139a752",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "h5tbx.File.create_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ad763-8935-4e89-a7ae-61f87b961b7b",
   "metadata": {},
   "source": [
    "A deep dive into what the various parameters to is given [here](..conventions/tbx.ipynb).\n",
    "\n",
    "But now let's create a sinusoidal signal $v(t)$, which represents a velocity measurement in `units` of volts. The measurments conversion factor into physical units for this example shall be $2.5 \\frac{m/s}{V}$. We choose \"vel\" as the dataset `name` but with `long_name` we will give a more precise description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc4043-cecc-4cc1-9297-484fa458973b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "time = np.linspace(0, np.pi/4, 21) # units [s]\n",
    "signal = np.sin(2*np.pi*3*time) # units [V], physical: [m/s]\n",
    "\n",
    "with h5tbx.File(contact='https://orcid.org/0000-0001-8729-0482') as h5:\n",
    "    vel_hdf_filename = h5.hdf_filename # store for later use\n",
    "    \n",
    "    ds_time = h5.create_dataset(name='time',\n",
    "                                data=time, \n",
    "                                units='s',\n",
    "                                long_name='measurement time',\n",
    "                                make_scale=True)\n",
    "    \n",
    "    ds_signal = h5.create_dataset(name='vel',\n",
    "                                  data=signal,\n",
    "                                  units='V',\n",
    "                                  offset=10.0,\n",
    "                                  scale='2.5 m/s/V',\n",
    "                                  long_name='air velocity in pipe',\n",
    "                                  attach_scale=ds_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b489d3-7024-44c2-92a4-b9e03b774c59",
   "metadata": {},
   "source": [
    "With the `h5rdmtoolbox` you receive a `xr.DataArray` object instead of a `np.ndarray` when an HDF5 dataset is sliced. Thus, meta information (the attributes of the dataset) is still provided with the data and useful features like plotting is possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b486d-4adc-47ce-89ac-42ff6c2f3d41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5tbx.File(vel_hdf_filename) as h5:\n",
    "    vel_data = h5['vel'][:]\n",
    "    vel_data.plot(marker='o')\n",
    "    \n",
    "vel_data  # this returns the interactive view of the array and its meta data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6369b961-15b0-4b0c-a483-3bffca2b7c20",
   "metadata": {},
   "source": [
    "## Create a group\n",
    "Groups don't really differ from the implementation in `h5py`. Besides standard attributes, which may be required, `attrs` can be passed during group creation. This is also possible for dataset creationg. Overwriting existing objects is possible, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0675041d-970f-4d86-bb01-477513a290f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5tbx.File(vel_hdf_filename, 'r+') as h5:\n",
    "    h5.create_group('mygroup',\n",
    "                    overwrite=True,\n",
    "                    attrs={'long_name': 'my special group'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1627e47a-37d1-45b2-8adc-d04d038b1574",
   "metadata": {},
   "source": [
    "## Natural Naming\n",
    "Until here we used the conventional way of addressing variables and groups in a dictionary-like style. `h5RDMtoolbox` allows to use \"natural naming\" which means that we can address those objects as if they were attributes. Make sure `h5tbx.config.natural_naming` is set to `True` (the default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c2dce-fd5e-43e1-98ef-ec6c6d98736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5rdmtoolbox import config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b678a5-403f-4055-9d1b-45153dcb7b58",
   "metadata": {},
   "source": [
    "Let's first disable `natural_naming`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224c1b74-bd60-4c2e-b498-1f3dab56aa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.natural_naming = False\n",
    "with h5tbx.File(vel_hdf_filename, 'r') as h5:\n",
    "    try:\n",
    "        ds = h5.vel[:]\n",
    "    except AttributeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3514ae51-07ca-4e97-a37f-bbd3a8c29f3d",
   "metadata": {},
   "source": [
    "Enable it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a088543-0338-4022-a691-4108a95b1951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config.natural_naming = True\n",
    "with h5tbx.File(vel_hdf_filename, 'r') as h5:\n",
    "    ds = h5.vel[:]\n",
    "    grp = h5.mygroup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcc735d-8422-4710-a794-984a59580ad6",
   "metadata": {},
   "source": [
    "## Inspect file content\n",
    "Often it is necessary to inspect the content of a file (structure, meta data, not the raw data). Calling `dump()` on a group represents the content (dataset, groups and attributes) as a pretty nd interactive (!) html representation. This is adopted from the `xarray` package. All credits for this idea go there. The representation here avoids showing data, though. Outside an IPython environment call `sdump()` to get a string representation of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6775f0-f3f0-40ac-887d-d00068188138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5tbx.File(vel_hdf_filename) as h5:\n",
    "    h5.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f5ebb1-2587-41f1-a9eb-1f52b9ba70de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5tbx.File(vel_hdf_filename) as h5:\n",
    "    h5.sdump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6879f372-a62a-4e9e-ad4d-268205503c72",
   "metadata": {},
   "source": [
    "## Conventions\n",
    "\n",
    "Conventions specify **which attributes are specified** and which of them are **required** for an HDF5 file. These specifications are called **standard attributes** and **layouts**:\n",
    "\n",
    "### Standard Attributes\n",
    "\n",
    "[Standard Attributes](../conventions/standard_attributes.ipynb) are defined via special python classes. They have a `get` and `set` method which check e.g. the syntax or the value of an attribute. These attributes can be associated with the root group, other groups or datasets. They can also be required during the creation of those objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ecf1d1-d2d8-4f30-b7d4-9aa6eef3cabb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from h5rdmtoolbox.conventions import StandardAttribute\n",
    "\n",
    "class CreationTime(StandardAttribute):\n",
    "    name = 'creation_time'\n",
    "    \n",
    "    def get(self):\n",
    "        return f'The creation time is: {super().get()}'\n",
    "    \n",
    "    def set(self, value):\n",
    "        if not isinstance(value, datetime.datetime):\n",
    "            raise ValueError(f'Not a valid creation time: {value}')\n",
    "        return super().set(str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51fddc2-19bf-4bcf-85c1-13b6f1602c5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from h5rdmtoolbox.conventions import Convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8deb6-6e33-408e-b012-0b705d921758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mycv = Convention('my-cv')\n",
    "mycv['__init__'].add(CreationTime,\n",
    "                     add_to_method=True,\n",
    "                     position={'after': 'mode'},\n",
    "                     optional=False)\n",
    "mycv.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a53f296-8fff-4392-ab76-94f4f8c08bd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "h5tbx.use('my-cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95985342-d4a5-4a59-8ebf-254620fa7e25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    with h5tbx.File() as h5:\n",
    "        h5.dump()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d60380-aa66-49ab-a6d3-2dcc3a4108ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "with h5tbx.File(creation_time=datetime.datetime.now()) as h5:\n",
    "    h5.dump()\n",
    "    print(h5.creation_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b88c46-3640-46fa-86f1-4a2b92fef688",
   "metadata": {},
   "source": [
    "### Layouts\n",
    "\n",
    "Layouts define how a file is expected to be orginzed, which groups and datasets must exist, which attributes are expected and much more. Layout define expectations an thus help file exchange where multiple users are involved. E.g. for numerical and experimental data, layouts are defined such that the minimum data will exist. If a layout validation exist, the exchanged file is rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12769f-f6d8-40d6-b352-01844767d0f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from h5rdmtoolbox.conventions import Validator\n",
    "\n",
    "class ValidCreationTime(Validator):\n",
    "    \n",
    "    def __init__(self, optional):\n",
    "        super().__init__(reference=None, optional=optional)\n",
    "    \n",
    "    def validate(self, date_string):\n",
    "        try:\n",
    "            dt = datetime.datetime.strptime(date_string, '%Y-%m-%d %H:%M:%S.%f')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bcb52f-c8f4-4ca3-8929-0dce1fb71d8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from h5rdmtoolbox.conventions import Layout, validators\n",
    "\n",
    "lay = h5tbx.conventions.layout.Layout()\n",
    "lay['/'].attrs['creation_time'] = ValidCreationTime(False)\n",
    "lay['/'].attrs['title'] = validators.ValidString(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b14e67-25f9-4b51-8e9f-7af4702b0fd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5tbx.File(creation_time=datetime.datetime.now()) as h5:    \n",
    "    lay.validate(h5)\n",
    "lay.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d5f216-121e-4eb5-b368-e49de2637cba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lay.get_failed_validations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8329f5f0-c483-46ee-bbeb-0d92d9e8d405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5tbx.File(creation_time=datetime.datetime.now()) as h5:    \n",
    "    h5.attrs['title'] = 'Test file'\n",
    "    lay.validate(h5)\n",
    "lay.report()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
