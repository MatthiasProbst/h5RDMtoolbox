{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26a6643f-3d3c-4cf4-99c7-32ddd5a4ec0a",
   "metadata": {},
   "source": [
    "# H5Database\n",
    "\n",
    "Three concepts are provided in the scope of this sub-package:\n",
    "- ``H5repo``: Using external HDF5 links. A HDF5 file serves as a \"table of content\" to link to files within a classic file system\n",
    "- ``H5Files``: Allows to open multiple files at the same time and brwose through them\n",
    "- ``h5mongo``: Using pymongo (mongodb) to mirrow meta data of hdf files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bdad1d-23d7-4076-8f03-aae3e8b636e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5rdmtoolbox import h5database as h5db\n",
    "from h5rdmtoolbox import generate_temporary_directory\n",
    "from h5rdmtoolbox.h5database import tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b4181-05be-4ef9-91c9-f92bc665b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tocdir = generate_temporary_directory('test_repo')\n",
    "tutorial.build_test_repo(tocdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4583afff-34a7-4712-9aae-2c68c11c1e84",
   "metadata": {},
   "source": [
    "## H5Repo - External link based reository\n",
    "\n",
    "Initialize a `H5Repo` object and specify the root directory under which HDF files are placed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0523ac-f8f2-40da-9479-429bc75d0ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = h5db.H5repo(tocdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f835c537-07f8-4429-b9ea-e66394ff51ef",
   "metadata": {},
   "source": [
    "The object creates a `toc` file (toc=table of content) which is a HDF5 file with external links to the found HDF files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aca164-705e-48db-86b4-aa7eefa7767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.toc_filename.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6c7c36-a3bc-4678-aeca-de08490586c1",
   "metadata": {},
   "source": [
    "The content can be dumped to the screen as a (pandas-) table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5676cc-feff-4d0e-b236-09a204353d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.dump(full_path=False)  # minimizes the output (no full folder path is shown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cb5748-4ff8-4ffd-9e30-31f487b1f8d1",
   "metadata": {},
   "source": [
    "The entries can be indexed and the file content is shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d3f15-1db0-4997-b1f4-b835709d497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5805f0c8-d760-4579-8c7d-4ddfaad4cddf",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "The repository can be filtered in a HDF5-like syntax. First import all filter classes from the module `filter_classes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fedee5-9c7e-4e3f-9696-122ae70f4b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5rdmtoolbox.h5database.filter_classes import *\n",
    "# repo.list_attribute_values('operator', '/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa147aa-2a4b-4e41-8da9-aa0a8a1212e3",
   "metadata": {},
   "source": [
    "The filter method requires an object `Entry`. It is the access location within a file, here the group \"operation_point\" in the root group. In the example the repository is filtered for the attribute \"long_name\" equal to \"Operation point data group\". A sub-repository is returend which is again an HDF5 file with external links - but this time only to the HDF files matching the filter request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f70588-c174-4866-92fa-b3c6726774f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sub_repo = repo.filter(Entry['/operation_point'].attrs['long_name'] == 'Operation point data group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671f20e4-f690-44e1-b61d-58e4e4c1afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_repo.dump(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35391fc0-61c4-4fd7-9d63-d962fc6e3701",
   "metadata": {},
   "source": [
    "The elsaped time for the filter request and building the new HDF toc-file is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e484ec26-9272-49e5-a470-ea446882f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_repo.elapsed_time  # [s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51047831-5045-449a-a221-cf7cfb90ae0d",
   "metadata": {},
   "source": [
    "Evaluating the sub-repository is quite straight forward as we are still working with HDF5 files. Let's plot data from the filter results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40f781d-4c3f-47a6-bf54-4d5f528fb1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "for r in sub_repo:\n",
    "    with r as h5:\n",
    "        if 'operation_point' in h5:\n",
    "            plt.scatter(h5['operation_point']['vfr'].attrs['mean'], h5['operation_point']['ptot'].attrs['mean'])\n",
    "plt.xlabel('vfr')\n",
    "plt.ylabel('ptot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e38b21-2b62-47b8-8db1-64c40b5435f5",
   "metadata": {},
   "source": [
    "## H5Files - Accessing multiple HDF files\n",
    "\n",
    "This concepts assumes that we already know the HDF files. This might be a result from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a00def5-4516-41ff-96bd-ed53366a9e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5rdmtoolbox.h5database import H5Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea3681-a384-46a2-93d4-17aa302bfa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_repo[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f369434-e5ce-470e-8225-22fdcab214c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with H5Files(*[sr.filename for sr in sub_repo[0:4]]) as h5files:\n",
    "    print(h5files.keys())\n",
    "    h5files[list(h5files.keys())[0]].dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d914df79-c941-4b63-978e-7ccbd4cfe11d",
   "metadata": {},
   "source": [
    "## HDF and PyMongo\n",
    "\n",
    "Last but not least `h5database` provides a \"real\" database solution using `pymongo`. Here, not the file sbut the meta informations are written to so-called `collections`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c96fc-62ac-4462-8e87-d131aeb52c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40e8640-b166-4792-913b-6ce39765eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3cb21-e1e3-4969-9103-a48ac49547b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client['h5database_notebook_tutorial']\n",
    "collection = db['test']\n",
    "collection.drop() # delete all entries if already exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f603e9-a72c-4d92-a645-3e5ae4584641",
   "metadata": {},
   "source": [
    "Import the mongo module (will add the accessor `mongo` to datasets and groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8741cd44-0b1b-4931-a26b-19530917f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5rdmtoolbox.h5database import mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d047f-6b02-4153-90e6-941165600dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5rdmtoolbox as h5tbx\n",
    "\n",
    "for fname in repo.filenames:\n",
    "    with h5tbx.H5File(fname) as h5:\n",
    "        h5.mongo.insert(collection=collection, recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ab4c31-b9bb-4c2a-84f2-8a3beb6bc1c2",
   "metadata": {},
   "source": [
    "Let's inspect the found database entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3f259a-a4e2-4ef1-9710-8938d1b4c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e6c58-6f5f-4423-850b-92a6f32298b6",
   "metadata": {},
   "source": [
    "Let's do the equivalent filter request as before (`sub_repo = repo.filter(Entry['/operation_point'].attrs['long_name'] == 'Operation point data group')`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf40c0-882d-406c-87fc-70b822d6fd21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "res = collection.find({'long_name': 'Operation point data group'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d293238-183a-4ff3-b7ca-b012bf7c0d1a",
   "metadata": {},
   "source": [
    "The number of found files are the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69da5b4-414b-4601-ad16-fb7b55cb0ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sub_repo), collection.count_documents({'long_name': 'Operation point data group'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed71e961-2563-430b-8608-298591ae8362",
   "metadata": {},
   "source": [
    "Let's generate the equivalent plot as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3452b14c-7020-448d-8670-34639b5f4c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plt.figure()\n",
    "\n",
    "for r in res.rewind():\n",
    "    with h5tbx.H5File(r['filename']) as h5:\n",
    "        if 'operation_point' in h5:\n",
    "            plt.scatter(h5['operation_point']['vfr'].attrs['mean'], h5['operation_point']['ptot'].attrs['mean'])\n",
    "plt.xlabel('vfr')\n",
    "plt.ylabel('ptot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc41829-ea39-4ffb-bfb3-4016a0053a51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
