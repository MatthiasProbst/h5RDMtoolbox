{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baf43fc3-c223-40b8-8949-b247a2883b85",
   "metadata": {},
   "source": [
    "# Specialized Wrappers\n",
    "\n",
    "By inheriting the main wrapper class `H5File` domain-specific functionality can be added. Although properties can be registered, it can be worth the effort of inheriting the class to permanently add features to it.\n",
    "\n",
    "The package provides two such class:\n",
    " - `H5Flow` and \n",
    " - `H5PIV`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4a8bd5-eb20-42c1-a365-4d84043137f6",
   "metadata": {},
   "source": [
    "## H5Flow\n",
    "\n",
    "To enhance the work with fluid-related HDF5 datasts, `H5Flow` is implemented, which is inherited from `H5File`. Thus, all features plus the domain specific one, that will be outlined here, are available.\n",
    "\n",
    "For fluid data, we expect the file to have certain datasets, that are coordinates and velocities. Most likely other derived datasets may exist like gradients or other physical measured or simulated variables like pressure, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbf2d86-3ed6-443c-a9e5-47ca2c242317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5rdmtoolbox as h5tbx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5b4200-7b82-4066-a975-423778ebe9ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5tbx.H5Flow() as h5:\n",
    "    filename = h5.hdf_filename  # keep for later use\n",
    "    h5.create_coordinates(x=np.linspace(0, 1, 20),\n",
    "                          y=np.linspace(0, 0.5, 10),\n",
    "                          z=np.linspace(-1, 1, 3),\n",
    "                          coords_unit='mm')\n",
    "    h5.create_velocity_datasets(u=np.random.rand(3, 10, 20),\n",
    "                                v=np.random.rand(3, 10, 20),\n",
    "                                w=np.random.rand(3, 10, 20),\n",
    "                                dim_scales=('z', 'y', 'x'),\n",
    "                                units='mm/s')\n",
    "    h5.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611871a0-44f5-4074-ab1c-db78d0d76a7d",
   "metadata": {},
   "source": [
    "## Accessing vector data\n",
    "\n",
    "Fluid data holds vector variables, like coordinates, velocities, gradients and more. To access them, the property `Vector` can be called. As it is generally unknown, which HDF datasets belong to a vector, we need to specify them like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab9b5c3-7d63-4ea3-9c2e-71fff0d3307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5Flow(filename) as h5:\n",
    "    vel = h5.Vector(names=('u', 'v', 'w'))[0:1, :, :]\n",
    "vel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8ea821-e50e-43fc-a161-c0363e72a475",
   "metadata": {},
   "source": [
    "A `xr.Dataset` is received, which we can work on, e.g. compute the magnitude and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f2c7dc-a8c0-4c6d-be1f-fd7de3f26e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vel.compute_magnitude()\n",
    "vel.magnitude.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b6724a-129a-4299-9a7c-161c25ca4264",
   "metadata": {},
   "source": [
    "As seen, the vector components can be addressed passing the keyword `names` in the call-statement. We could also use the keyword `standard_name`.\n",
    "To make use of the `standard_name` attribute, `VelocityDataset` is the specialized version of the `Vector` property. If sarches for the keywords `x_velocity`, `y_velocity` and `z_velocity`. This approach obviously only works if those attribtues are unique within the HDF group. If not, `name` can still be passed in the call-statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fc2956-2fe6-438e-a9f3-1b11e8e89271",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5Flow(filename) as h5:\n",
    "    vel_uvw = h5.VelocityVector[0:1, :, :]\n",
    "    vel_uv = h5.VelocityVector(names=('u', 'v'))[0:1, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77788005-93e7-4e1a-8011-7b05431c2d2c",
   "metadata": {},
   "source": [
    "## Connecting datast to a devices\n",
    "Assume we have experiment data, e.g. we measured the pressure. We can reference the measurement device in the dataset like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1caf163-9aed-496d-ada8-1ac02f6b04f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5Flow() as h5:\n",
    "    h5.create_group('devices')\n",
    "    device_grp = h5.create_group('devices/PressureSensor')\n",
    "    device_grp.attrs['manufacturer'] = 'unknown'\n",
    "    device_grp.create_dataset('x', data=0, units='m', standard_name='x_coordinate')\n",
    "    device_grp.create_dataset('y', data=0, units='m', standard_name='y_coordinate')\n",
    "    device_grp.create_dataset('z', data=0, units='m', standard_name='z_coordinate')\n",
    "    ds = h5.create_dataset('pressure', data=np.random.random(100), units='Pa', standard_name='pressure',\n",
    "                          device=device_grp)\n",
    "    print(ds.device)\n",
    "    print(type(ds.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e304651-6b5a-481b-9876-04ab88dbba0d",
   "metadata": {},
   "source": [
    "**Alternative 1:** We can first use the `Device` class to create an object, which we write to the hdf group and then create the reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a127c86-a43c-443d-92dc-c3bbf6a516e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sensor = h5tbx.h5wrapper.h5flow.Device('PressureSensor', manufacturer='unknown',\n",
    "                  x=(0, dict(units='m',standard_name='x_coordinate')),\n",
    "                  y=(0, dict(units='m',standard_name='y_coordinate')),\n",
    "                  z=(0, dict(units='m',standard_name='z_coordinate')))\n",
    "p_sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72593880-6444-4e2b-b991-2302e15a4ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5Flow() as h5:\n",
    "    devices_grp = h5.create_group('devices')\n",
    "    sensor_grp = p_sensor.to_hdf_group(devices_grp)\n",
    "    \n",
    "    ds = h5.create_dataset('pressure', data=np.random.random(100), units='Pa', standard_name='pressure',\n",
    "                          device=sensor_grp)\n",
    "    ds.device = sensor_grp\n",
    "    print(ds.device)\n",
    "    print(type(ds.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e188c606-9e9e-491e-a564-fe4b0c04095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5Flow() as h5:    \n",
    "    ds = h5.create_dataset('pressure', data=np.random.random(100), units='Pa', standard_name='pressure')\n",
    "    ds.device = p_sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f2e27-a98f-433c-b862-de950ba36ccf",
   "metadata": {},
   "source": [
    "---\n",
    "## H5PIV\n",
    "\n",
    "Velocity is a vector quantity. In the HDF file each component generally is stored as an individual data variable. To get the full vector in one variable the attribute `VelocityVector` can be called. Slicing this object will assigned it with the respectve arrays from the HDF datasets. In this case x- and y-velociy datasets are siced and merged into a `xr.Dataset`. The `VelocityVector` class is wrapped around the `xr.Dataset` class and has additional methods, like `compute_magnitude()`.\n",
    "\n",
    "Note, that `VelocityVector` is not a property of `H5PIV` by default but is added afterwards, similar to how dataarray-ccessors are added to an `xr.DataArray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6547baf5-d41f-4a4f-8369-6ee838b54103",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.tutorial.get_H5PIV('minimal_flow', mode='r') as h5:\n",
    "    vel = h5.VelocityVector[:]\n",
    "vel.compute_magnitude()\n",
    "vel.magnitude[0,0,:,:].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50bea0d-8053-419c-b7a6-c220d5fc1a87",
   "metadata": {},
   "source": [
    "### Post-processing with `H5PIV`\n",
    "\n",
    "Based on standard names velocity, velocity gradients and other variables are identified. Each group therefore should have only one velocity vector, otherwise it is not clear from which to compute e.g. the turbulent kinetic energy.<br>\n",
    "From each group, multiple fluid-specific post-processing methods can be called. They store the result in the respective group if required data was correcly identified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f3b2e-5d5c-45c2-882b-bffee7fd24e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.tutorial.get_H5PIV('minimal_flow', mode='r+') as h5:\n",
    "    rm = h5.compute_running_mean('u')\n",
    "    rs = h5.compute_running_std('u')\n",
    "    (rs[0,-1,:,:]/rm[0,-1,:,:]).plot(vmin=-2, vmax=2)\n",
    "    h5.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76143c4-9188-4a72-ae10-6c800a12859e",
   "metadata": {},
   "source": [
    "### Valid vector detection probability (vdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b38a7-155f-49b3-9a6e-b5201fbbfa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.tutorial.get_H5PIV('vortex_snapshot', 'r+') as h5:\n",
    "    h5['piv_flags'].attrs.rename('flag_translation', 'flag_meanings')\n",
    "    h5.get_dataset_by_standard_name('piv_flag').compute_vdp()\n",
    "    h5.dump()\n",
    "    print(h5.post.vdp[()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52700ee-0fcf-4973-9ddc-cd9d509340e3",
   "metadata": {},
   "source": [
    "## Computing PIV uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8860ffb2-55d7-43aa-b4fc-24e3b1175967",
   "metadata": {},
   "source": [
    "Get an example HDF filename from the ILA vortex pair example (https://www.pivtec.com/pivview.html):\n",
    "\n",
    "To compute the uncertainty of a PIV measurement, we need to gather some specific datasets, namely the at minimum the pixel coordinates, the displacements and the raw images. The class property `UncertaintyDataset` does this for us. Calling it will return a `xarray.Dataset` with the displacement variables.\n",
    "\n",
    "Let's load the vortex example and fetch image A and imabe B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b6e2e-685a-4460-a4f9-7c47df916527",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.tutorial.get_H5PIV('vortex_snapshot', 'r+') as h5:\n",
    "    disp = h5.DisplacementVector[:,:]\n",
    "    imgA = h5.imgA[:,:]\n",
    "    imgB = h5.imgB[:,:]\n",
    "disp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7cc1b0-4fe4-48f0-96f4-b02a72e7b253",
   "metadata": {},
   "source": [
    "The uncertainty dataset has the coordinates `x` and `y`, the displacements arrays `dx` and `dy` but also the pixel coordinates `ix` and `iy`\n",
    "\n",
    "Next, let's create a more or less random uncertainty method. In this example we do not compute the real error but assume one, just to explain the workflow of cumputing the uncertainty from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3625b6b5-4dba-419a-ada2-3e0c01e1f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_uncertainty_method(uds, imgA, imgB):\n",
    "    \"\"\"\n",
    "    Dummy uncertainty method for this tutorial.\n",
    "    Returns the same dataset but with added uncertainties\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    uds: XRUncertaintyDataset\n",
    "        The uncertainty dataset containing, x, y, ix, iy, dx, dy, ...\n",
    "    imgA: np.ndarray\n",
    "        2d PIV image A. Will not be touch in this example\n",
    "    imgB: np.ndarray\n",
    "        2d PIV image B. Will not be touch in this example\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    uds: XRUncertaintyDataset    \n",
    "    \"\"\"\n",
    "    import xarray as xr\n",
    "    xerr = 0.05\n",
    "    yerr = 0.075\n",
    "    udx = np.abs(uds.dx)*xerr\n",
    "    uds['udx'] = xr.DataArray(dims=uds.dx.dims, data=udx,\n",
    "                                        attrs={'standard_name': f'uncertainty_of_{uds.dx.standard_name}',\n",
    "                                               'units': 'pixel',\n",
    "                                               'piv_uncertainty_method': 'my_uncertainty_method'})\n",
    "    udy = np.abs(uds.dy)*yerr\n",
    "    uds['udy'] = xr.DataArray(dims=uds.dy.dims, data=udy,\n",
    "                                        attrs={'standard_name': f'uncertainty_of_{uds.dy.standard_name}',\n",
    "                                               'units': 'pixel',\n",
    "                                               'piv_uncertainty_method': 'my_uncertainty_method'})\n",
    "    return uds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f7324-3ea4-468b-b967-978e712295c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "un = disp.compute_uncertainty(my_uncertainty_method, imgA, imgB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ba5274-828d-4c9f-9672-afec8c1dbfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "un.compute_magnitude()\n",
    "_ = un.magnitude[:].plot.contourf(vmax=6, vmin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56e337f-a998-4341-b4ac-fff1c14e1191",
   "metadata": {},
   "outputs": [],
   "source": [
    "udx = un.get_by_standard_name('uncertainty_of_x_displacement')\n",
    "_ = udx.where(np.abs(udx) < 20).plot.contourf()\n",
    "print(f'Error in x-direction: {udx.mean().values}')\n",
    "print(f'Absolute relative error in x-direction: {np.divide(udx, np.abs(un.dx)).mean().values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07657666-4b2c-41ba-ac8d-b03bc9e95585",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5tbx.conventions.identifier.STRICT = False\n",
    "# uncertainty_of_x_displacement is not part of the standard name table at this moment. \n",
    "# Don't check if standard names are in the respective table. this can be done by diabeling the \"strictness\" of checking standard names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e58ce-a211-421d-a010-bf7f574d4189",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5PIV(h5.hdf_filename, 'r+') as h5:\n",
    "    h5.create_group('uncertainty', overwrite=True)\n",
    "    h5['uncertainty'].create_dataset('delta_dx', data=un.get_by_standard_name('uncertainty_of_x_displacement'), overwrite=True)\n",
    "    h5['uncertainty'].create_dataset('delta_dy', data=un.get_by_standard_name('uncertainty_of_x_displacement'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b0fc38-7da4-494f-891c-2db252720a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5PIV(h5.hdf_filename, 'r+') as h5:\n",
    "    h5.dump()\n",
    "    h5.uncertainty.delta_dx[:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1bf800-1029-4945-b453-136888dd6090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
